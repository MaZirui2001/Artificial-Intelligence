# HW 08

## 1

> 试证明对于不含冲突数据集（即特征向量完全相同但标记不同）的训练集，必存在与训练集一致（即训练误差为 0）的决策树。

我们可以采用递归的方式来构建这样的一棵与训练集一致（即训练误差为 0）的决策树。

首先，选择一个划分属性，将数据集根据该属性划分为若干个子集。然后，在每个子集中递归构建决策树，直到所有子集中的数据都属于同一类别或者无法再选择划分属性为止。

由于数据集不含冲突数据，因此在每个子集中，所有的数据都属于同一类别。因此，递归构建决策树的过程中，每个叶子节点都对应于一个数据的类别，而且每个叶子节点的数据集都只包含属于该类别的数据。

因此，我们可以构建这样的一个与训练集一致的决策树：每个叶子节点对应于一个数据的类别，而且每个非叶子节点对应于一个划分属性，将数据集划分为若干个子集。由于数据集不含冲突数据，因此在每个子集中，所有的数据都属于同一类别。这样的决策树可以将每个特征向量映射到其正确的类别，训练误差为 0。

## 2

> 最小二乘学习方法在求解$min_w(Xw-y)$问题后得到闭式解 $w^*=(X^TX)^{-1}X^Ty$（为简化问题，我们忽略偏差项 $b$）。如果我们知道数据中部分特征有较大的误差，在不修改损失函数的情况下，引入规范化项$\lambda w^TDw$,其中$D$为对角矩阵，由我们取值。相应的最小二乘分类学习问题转换为以下形式的优化问题：
>
> $min_w(Xw-y)+\lambda w^TDw$
>
> **(1).**请说明选择规范化项$w^TDw$而非$L_2$规范化项$w^Tw$的理由是什么。$D$的对角线元素$D_{ii}$有何意义，它的取值越大意味着什么?
>
> **(2).**请对以上问题进行求解。

### (1)

题目中提出数据的部分特征有较大的误差，选择规范化项$w^TDw$是因为$w^TDw$可以对不同特征的权重进行不同程度的约束，从而避免某些特征对预测结果的贡献过大。而$L_2$规范化项$w^Tw$对所有特征权重的平方和进行约束，无法区分不同特征的重要性。

$D$的对角线元素$D_{ii}$表示对第$i$个特征权重的约束程度，取值越大则表示该特征的权重受到的约束越强，即该特征对预测结果的贡献越小。

### (2)

将规范化项引入后，最小二乘分类学习问题的优化目标变为：
$$
\min_w\left(|Xw-y|^2+\lambda w^TDw\right)
$$
为了求解上述优化问题，我们可以先对$w$进行展开：
$$
w=\begin{bmatrix}w_1\\w_2\\ \cdots\\w_d\end{bmatrix}
$$
其中，$d$为特征的数量。则有：
$$
\begin{aligned}
\lambda w^TD w 
&=\lambda\sum_{i=1}^d w_i^2D_{ii} \\ 
&=w^T\begin{bmatrix}\lambda D_{11} & 0 & \cdots & 0 \\
0 & \lambda D_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda D_{dd}\end{bmatrix}w\\
&=w^T\Lambda w\end{aligned}
$$


其中，$\Lambda$即为第二步中的对角矩阵，其对角线元素为$\lambda D_{ii}$。

此时问题转化为求解：
$$
\min_w\left(|Xw-y|^2+w^T\Lambda w\right)
$$
对$w$求导，令导数为0，可得：
$$
X^TXw - X^Ty+\Lambda w=0 \\
w^*=(X^TX+\Lambda)^{-1}X^Ty
$$

## 3

> 假设有$n$个数据点$x_1, \dots, x_n$以及一个映射$\varphi:x\rightarrow\varphi(x)$ ,以此定义核函数$K(x, x')=\varphi(x) \cdot\varphi(x')$。试证明由该核函数所决定的核矩阵$K: K_{i,j}=K(x_i, x_j)$有以下性质:
>
> **(1).** $K$是一个对称矩阵;
>
> **(2).** $K$是一个半正定矩阵，即$\forall z \in R^n, z^TKz \ge 0$。

### (1)

证明：对于 $\forall i, j \in [n]$，对于矩阵$K$我们有：
$$
\begin {align}
K_{i, j} &= \varphi(x_i) \cdot \varphi(x_j) \\
&= \varphi(x_j) \cdot \varphi(x_i) \\
&= K_{j, i}
\end {align}
$$
因此$K$是对称矩阵

### (2)

将$K$做矩阵展开，并把二次型写为和式：
$$
\begin {align}
z^TKz &= \sum_{i = 1}^n\sum_{j=1}^nz_iK_{i, j}z_j \\
&= \sum_{i = 1}^n\sum_{j=1}^nz_i\varphi(x_i)^T\varphi(x_j)z_j \\
&= \sum_{i = 1}^nz_i\varphi(x_i)^T \sum_{j=1}^nz_j\varphi(x_j) \\
&= ||\sum_{i = 1}^nz_i\varphi(x_i)||^2 \\
&\ge 0
\end {align}
$$
由此得证。

## 4

> $K-means$算法是否一定会收敛? 如果是，给出证明过程；如果不是，给出说明

该算法最后会收敛，证明需要针对算法的两个步骤作证明：

* 算法第一步会找到将所有的点定位到距离其最近的中心点，对于任意的点，这样处理后会让其距离中心点的距离更小，因为新定位的中心点距离它的距离是所有中心点中最小的，这样更新的距离一定会比这个点上次的距离要短，因此所有点和中心点的距离在这一步必定减小。

* 算法第二步重新确定了每个聚类的中心点，我们需要证明：对于任意聚类$A$和其中的点$a_1, ..., a_n$，定义其新中心点$C(A) = \frac1A \sum_{i = 1}^na_n$，则对于任意的点$x$， $\sum_{i=1}^n|a_i-C(A)|^2 \le \sum_{i=1}^n|a_i-x|^2$。

  下面来对右边的的式子进行变换：
  $$
  \begin {align}
  \sum_i |a_i-x|^2 &= \sum_i|a_i-C(A)+C(A) - x|^2 \\
  &= \sum_i |a_i-C(A)|^2 + \sum_i (a_i-C(A))\cdot (C(A)-x) +\sum_i|C(A)-x|^2 \\
  &= \sum_i |a_i-C(A)|^2 + (C(A)-x) \cdot \sum_i (a_i-C(A)) + |A||C(A)-x|^2 \\
  
  &= \sum_i |a_i-C(A)|^2 + |A||C(A)-x|^2 \\
  &\ge \sum_i |a_i-C(A)|^2
  
  \end {align}
  $$
  其中倒数第二步用到了$\sum_i (a_i-C(A)) = 0$，定理得证